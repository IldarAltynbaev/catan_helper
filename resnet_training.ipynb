{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import StratifiedKFold\nimport os\n\nimport torch\nimport numpy as np\nimport torchvision\nimport matplotlib.pyplot as plt\nimport time\nimport copy\nimport random\nfrom PIL import Image\n\nfrom torchvision import transforms, models\nfrom torchvision.transforms import functional\nfrom torch.utils.data import ConcatDataset\nfrom matplotlib.animation import FuncAnimation\n\nimport shutil \nfrom tqdm import tqdm\n\ndef set_random_seed(random_seed):\n    torch.manual_seed(random_seed)\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n\ndef fill_list_of_classes(wood_count, wheat_count, stone_count, sheep_count, brick_count):\n    list_of_classes = []\n    for i in range(wood_count):\n        list_of_classes.append(0)\n    for i in range(wheat_count):\n        list_of_classes.append(1)\n    for i in range(stone_count):\n        list_of_classes.append(2)\n    for i in range(sheep_count):\n        list_of_classes.append(3)\n    for i in range(brick_count):\n        list_of_classes.append(4)\n\n    return list_of_classes\n\ndef fill_list_of_images(class_names, list_of_images):\n\n    for class_name in class_names:   \n        for file in os.listdir('C:/catan_github/catan_helper/resources_dataset/train/'+class_name):\n            file_name = os.fsdecode(file)\n            if (file.endswith(\".png\")):             \n                list_of_images.append(file_name)\n\ndef count_files(path_dir):\n    cpt = sum([len(files) for r, d, files in os.walk(path_dir)])\n    wood_count = sum([len(files) for r, d, files in os.walk(path_dir + \"/wood\")])\n    wheat_count = sum([len(files) for r, d, files in os.walk(path_dir+ \"/wheat\")])\n    stone_count = sum([len(files) for r, d, files in os.walk(path_dir + \"/stone\")])\n    brick_count = sum([len(files) for r, d, files in os.walk(path_dir + \"/brick\")])\n    sheep_count = sum([len(files) for r, d, files in os.walk(path_dir + \"/sheep\")])\n    print(wood_count, wheat_count, stone_count, sheep_count, brick_count, cpt)\n    return wood_count, wheat_count, stone_count, sheep_count, brick_count\n\ndef fill_val_folders(list_of_images,list_of_classes,train_dir,val_dir):\n    \n    strat_kfold = StratifiedKFold(n_splits=5, shuffle=True)\n    for fold, (train_ids, test_ids) in enumerate(strat_kfold.split(list_of_images,list_of_classes)):\n        \n        if fold == 0:\n            for each_id in test_ids:\n                if list_of_classes[each_id] == 0:\n                    shutil.move(train_dir + '/wood/'+list_of_images[each_id], val_dir+'/wood/'+list_of_images[each_id])    \n                elif list_of_classes[each_id] == 1:\n                    shutil.move(train_dir + '/wheat/'+list_of_images[each_id], val_dir+'/wheat/'+list_of_images[each_id])\n                elif list_of_classes[each_id] == 2:\n                    shutil.move(train_dir + '/stone/'+list_of_images[each_id], val_dir+'/stone/'+list_of_images[each_id])\n                elif list_of_classes[each_id] == 3:\n                    shutil.move(train_dir + '/sheep/'+list_of_images[each_id], val_dir+'/sheep/'+list_of_images[each_id])\n                elif list_of_classes[each_id] == 4:\n                    shutil.move(train_dir + '/brick/'+list_of_images[each_id], val_dir+'/brick/'+list_of_images[each_id])\n\ndef create_blank_model(device, freeze_layers = True):\n    model = models.resnet34(pretrained=True)\n    if freeze_layers:\n        for param in model.parameters():\n            #if isinstance(param, torch.nn.Conv2d):\n                param.requires_grad = False\n\n    model.fc = torch.nn.Linear(model.fc.in_features, 5)    \n    model = model.to(device)\n    return model\n\n\ndef create_train_transforms():\n\n    train_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.CenterCrop(60),\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])   \n    return train_transforms\n\ndef create_val_transforms():\n    val_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.CenterCrop(60),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n    return val_transforms\n\n\ndef create_optimizer(model,learning_rate):\n    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n    return optimizer\n    \ndef create_scheduler(optimizer,step_size,gamma):\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n    return optimizer\n\n\ndef get_train_val_dataloaders(num_of_augmentaions,train_dir,val_dir,batch_size):","metadata":{"_uuid":"c4048beb-b98b-4889-ae94-f0b6ab062f56","_cell_guid":"dc322dd4-173d-49b4-90f6-8b49dea5c6ac","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(device,model, loss,optimizer, \n                scheduler,accuracy_history_train,accuracy_history_val,\n                epoch_history,train_dataloader,val_dataloader, num_epochs):    \n    best_val_acc = 0\n    best_train_acc = 0\n    for epoch in tqdm(range(num_epochs)):\n        \n        epoch_history.append(epoch)\n        #print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                dataloader = val_dataloader\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.\n            running_acc = 0.\n\n            # Iterate over data.\n            for inputs, labels in dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                # forward and backward\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss_value.item()\n                #print(preds_class)\n                #print(labels.data)\n                running_acc += (preds_class == labels.data).float().mean()\n\n            epoch_loss = running_loss / len(dataloader)\n            epoch_acc = running_acc / len(dataloader)\n            if phase == 'train':\n                accuracy_history_train.append(epoch_acc.item())\n            else:\n                if epoch_acc.item() >= best_val_acc:\n                    #print('best model - ' + str(epoch_acc.item()) + ' val accuracy')\n                    torch.save(model.state_dict(),\n                               'C:/catan_github/catan_helper/resources_dataset/best_models/' + str(epoch) +'.pt')\n                    best_val_acc = epoch_acc.item() \n                \n                    #print(model.state_dict())\n                    #best_model = model.state_dict()\n                accuracy_history_val.append(epoch_acc.item())\n\n            #print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"set_random_seed(0)\n\ntrain_dir = 'C:/catan_github/catan_helper/resources_dataset/train'\nval_dir = 'C:/catan_github/catan_helper/resources_dataset/val'\nbatch_size = 8\nbest_model = None\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nlist_of_images = []\nclass_names = ['wood','wheat','stone','sheep','brick']\nwood_count, wheat_count, stone_count, sheep_count, brick_count = count_files('C:/catan_github/catan_helper/resources_dataset/train/')\n\nlist_of_classes = fill_list_of_classes(wood_count, wheat_count, stone_count, sheep_count, brick_count)\nfill_list_of_images(class_names, list_of_images)\n#fill_val_folders(list_of_images,list_of_classes,train_dir,val_dir)\n\ncount_files('C:/catan_github/catan_helper/resources_dataset/train/')\ncount_files('C:/catan_github/catan_helper/resources_dataset/val/')\n\ntr_transforms = create_train_transforms()\nlearning_rate =1.0e-3\nstep_size=7\ngamma=0.1\nmodel_name = 'Resnet34'\nmodel_freezed = 'except last layer'\nnum_of_epochs = 60\nnum_of_augmentaions = 1\noptimizer_name = 'Adam'\n\ntrain_dataloader,val_dataloader = get_train_val_dataloaders(num_of_augmentaions,\n                                                            train_dir,\n                                                            val_dir,\n                                                            batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = create_blank_model(device)\nloss = torch.nn.CrossEntropyLoss()  \noptimizer = create_optimizer(model,learning_rate)\nscheduler = create_scheduler(optimizer,step_size,gamma)\ntrain_dataloader,val_dataloader = get_train_val_dataloaders(num_of_augmentaions,\n                                                            train_dir,\n                                                            val_dir,\n                                                            batch_size)\n\nepoch_history = []\naccuracy_history_train = []\naccuracy_history_val = []\ntrain_model(device,model,\n            loss, \n            optimizer, \n            scheduler, \n            accuracy_history_train,\n            accuracy_history_val,\n            epoch_history,\n            train_dataloader,\n            val_dataloader,\n            num_of_epochs)\n\nprint(max(accuracy_history_train))\nprint(max(accuracy_history_val))\nplt.figure(figsize=(20,12))\nplt.plot(epoch_history ,accuracy_history_train,label = 'train')\nplt.plot(epoch_history ,accuracy_history_val,label = 'val')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for ind, (inputs, labels) in enumerate(train_dataloader):\n    #print(inputs.shape)\n    y_onehot = torch.zeros(batch_size, len(class_names))\n    y_onehot.scatter_(1, labels.unsqueeze(dim=1), 1)\n    print(y_onehot)\n    print(y_onehot.shape)\n    print(labels)\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}