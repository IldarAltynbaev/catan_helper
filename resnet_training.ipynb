{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12187350,"sourceType":"datasetVersion","datasetId":7676425}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import StratifiedKFold\nimport os\n\nimport torch\nimport numpy as np\nimport torchvision\nimport matplotlib.pyplot as plt\nimport time\nimport copy\nimport random\nfrom PIL import Image\n\nfrom torchvision import transforms, models\nfrom torchvision.transforms import functional\nfrom torch.utils.data import ConcatDataset\nfrom matplotlib.animation import FuncAnimation\n\nimport shutil \nfrom tqdm import tqdm\n\ndef set_random_seed(random_seed):\n    torch.manual_seed(random_seed)\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n\ndef fill_list_of_classes(wood_count, wheat_count, stone_count, sheep_count, brick_count,unknown_count):\n    list_of_classes = []\n    for i in range(wood_count):\n        list_of_classes.append(0)\n    for i in range(wheat_count):\n        list_of_classes.append(1)\n    for i in range(stone_count):\n        list_of_classes.append(2)\n    for i in range(sheep_count):\n        list_of_classes.append(3)\n    for i in range(brick_count):\n        list_of_classes.append(4)\n    for i in range(unknown_count):\n        list_of_classes.append(5)\n\n    return list_of_classes\n\ndef fill_list_of_images(class_names, list_of_images):\n\n    for class_name in class_names:   \n        for file in os.listdir('/kaggle/input/catan-resource-dataset/catan_resource_dataset/train/'\n                               +class_name):\n            file_name = os.fsdecode(file)\n            if (file.endswith(\".png\")):             \n                list_of_images.append(file_name)\n\ndef count_files(path_dir):\n    cpt = sum([len(files) for r, d, files in os.walk(path_dir)])\n    wood_count = sum([len(files) for r, d, files in os.walk(path_dir + \"/wood\")])\n    wheat_count = sum([len(files) for r, d, files in os.walk(path_dir+ \"/wheat\")])\n    stone_count = sum([len(files) for r, d, files in os.walk(path_dir + \"/stone\")])\n    brick_count = sum([len(files) for r, d, files in os.walk(path_dir + \"/brick\")])\n    sheep_count = sum([len(files) for r, d, files in os.walk(path_dir + \"/sheep\")])\n    unknown_count = sum([len(files) for r, d, files in os.walk(path_dir + \"/unknown\")])\n    \n    print(wood_count, wheat_count, stone_count, sheep_count, brick_count, unknown_count, cpt)\n    return wood_count, wheat_count, stone_count, sheep_count, brick_count, unknown_count\n\ndef fill_val_folders(list_of_images,list_of_classes,train_dir,val_dir):\n    \n    strat_kfold = StratifiedKFold(n_splits=5, shuffle=True)\n    for fold, (train_ids, test_ids) in enumerate(strat_kfold.split(list_of_images,list_of_classes)):\n        \n        if fold == 0:\n            for each_id in test_ids:\n                if list_of_classes[each_id] == 0:\n                    shutil.move(train_dir + '/wood/'+list_of_images[each_id], val_dir+'/wood/'+list_of_images[each_id])    \n                elif list_of_classes[each_id] == 1:\n                    shutil.move(train_dir + '/wheat/'+list_of_images[each_id], val_dir+'/wheat/'+list_of_images[each_id])\n                elif list_of_classes[each_id] == 2:\n                    shutil.move(train_dir + '/stone/'+list_of_images[each_id], val_dir+'/stone/'+list_of_images[each_id])\n                elif list_of_classes[each_id] == 3:\n                    shutil.move(train_dir + '/sheep/'+list_of_images[each_id], val_dir+'/sheep/'+list_of_images[each_id])\n                elif list_of_classes[each_id] == 4:\n                    shutil.move(train_dir + '/brick/'+list_of_images[each_id], val_dir+'/brick/'+list_of_images[each_id])\n                elif list_of_classes[each_id] == 5:\n                    shutil.move(train_dir + '/unknown/'+list_of_images[each_id], val_dir+'/unknown/'+list_of_images[each_id])\n\ndef create_blank_model(device, freeze_layers = True):\n    model = models.resnet34(pretrained=True)\n    if freeze_layers:\n        for param in model.parameters():\n            #if isinstance(param, torch.nn.Conv2d):\n                param.requires_grad = False\n\n    model.fc = torch.nn.Linear(model.fc.in_features, 6)    \n    model = model.to(device)\n    return model\n\n\ndef create_train_transforms():\n\n    train_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            #transforms.CenterCrop(60),\n            #transforms.Resize((224, 224)),\n            #transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])   \n    return train_transforms\n\ndef create_val_transforms():\n    val_transforms = transforms.Compose([\n            transforms.Resize((224, 224)),\n            #transforms.CenterCrop(60),\n            #transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n    return val_transforms\n\n\ndef create_optimizer(model,learning_rate):\n    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) \n    return optimizer\n    \ndef create_scheduler(optimizer,step_size,gamma):\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n    return optimizer\n\n\ndef get_train_val_dataloaders(num_of_augmentaions,train_dir,val_dir,batch_size):\n    list_of_train_datasets = []        \n\n    for i in range(num_of_augmentaions):\n        \n        train_transforms = create_train_transforms()\n        train_dataset = torchvision.datasets.ImageFolder(train_dir,train_transforms)   \n        \n        list_of_train_datasets.append(train_dataset)\n            \n    train_dataset = ConcatDataset(list_of_train_datasets)\n\n    val_transforms = create_val_transforms()\n    val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n    val_dataloader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size,drop_last=True)\n\n    return train_dataloader, val_dataloader","metadata":{"_cell_guid":"dc322dd4-173d-49b4-90f6-8b49dea5c6ac","_uuid":"c4048beb-b98b-4889-ae94-f0b6ab062f56","collapsed":false,"execution":{"iopub.status.busy":"2025-06-16T18:25:23.159365Z","iopub.execute_input":"2025-06-16T18:25:23.159657Z","iopub.status.idle":"2025-06-16T18:25:23.176856Z","shell.execute_reply.started":"2025-06-16T18:25:23.159635Z","shell.execute_reply":"2025-06-16T18:25:23.176165Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train_model(device,model, loss,optimizer, \n                scheduler,accuracy_history_train,accuracy_history_val,\n                epoch_history,train_dataloader,val_dataloader, num_epochs):    \n    best_val_acc = 0\n    best_train_acc = 0\n    for epoch in tqdm(range(num_epochs)):\n        \n        epoch_history.append(epoch)\n        #print('Epoch {}/{}:'.format(epoch, num_epochs - 1), flush=True)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                dataloader = val_dataloader\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.\n            running_acc = 0.\n\n            # Iterate over data.\n            for inputs, labels in dataloader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                # forward and backward\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss_value.item()\n                #print(preds_class)\n                #print(labels.data)\n                running_acc += (preds_class == labels.data).float().mean()\n\n            epoch_loss = running_loss / len(dataloader)\n            epoch_acc = running_acc / len(dataloader)\n            if phase == 'train':\n                accuracy_history_train.append(epoch_acc.item())\n            else:\n                if epoch_acc.item() >= best_val_acc:\n                    #print('best model - ' + str(epoch_acc.item()) + ' val accuracy')\n                    torch.save(model.state_dict(),\n                               '/kaggle/working/best_model.pt')\n                    best_val_acc = epoch_acc.item() \n                \n                    #print(model.state_dict())\n                    #best_model = model.state_dict()\n                accuracy_history_val.append(epoch_acc.item())\n\n            #print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2025-06-16T18:25:25.901357Z","iopub.execute_input":"2025-06-16T18:25:25.901957Z","iopub.status.idle":"2025-06-16T18:25:25.909241Z","shell.execute_reply.started":"2025-06-16T18:25:25.901934Z","shell.execute_reply":"2025-06-16T18:25:25.908450Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"set_random_seed(0)\n\ntrain_dir = '/kaggle/input/catan-resource-dataset/catan_resource_dataset/train/'\nval_dir = '/kaggle/input/catan-resource-dataset/catan_resource_dataset/val/'\nbatch_size = 8\nbest_model = None\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nlist_of_images = []\nclass_names = ['wood','wheat','stone','sheep','brick', 'unknown']\nwood_count, wheat_count, stone_count, sheep_count, brick_count, unknown_count = count_files(\n    train_dir)\n\nlist_of_classes = fill_list_of_classes(wood_count, wheat_count, stone_count, sheep_count, brick_count, unknown_count)\nfill_list_of_images(class_names, list_of_images)\n#fill_val_folders(list_of_images,list_of_classes,train_dir,val_dir)\n\ncount_files(train_dir)\ncount_files(val_dir)\n\ntr_transforms = create_train_transforms()\nlearning_rate =1.0e-3\nstep_size=7\ngamma=0.1\nmodel_name = 'Resnet34'\nmodel_freezed = 'except last layer'\nnum_of_epochs = 60\nnum_of_augmentaions = 1\noptimizer_name = 'Adam'\n\ntrain_dataloader,val_dataloader = get_train_val_dataloaders(num_of_augmentaions,\n                                                            train_dir,\n                                                            val_dir,\n                                                            batch_size)","metadata":{"execution":{"iopub.status.busy":"2025-06-16T18:25:28.636775Z","iopub.execute_input":"2025-06-16T18:25:28.637040Z","iopub.status.idle":"2025-06-16T18:25:28.953956Z","shell.execute_reply.started":"2025-06-16T18:25:28.637021Z","shell.execute_reply":"2025-06-16T18:25:28.953158Z"},"trusted":true},"outputs":[{"name":"stdout","text":"81 77 55 54 72 36 375\n81 77 55 54 72 36 375\n24 13 16 22 24 6 105\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model = create_blank_model(device)\nloss = torch.nn.CrossEntropyLoss()  \noptimizer = create_optimizer(model,learning_rate)\nscheduler = create_scheduler(optimizer,step_size,gamma)\ntrain_dataloader,val_dataloader = get_train_val_dataloaders(num_of_augmentaions,\n                                                            train_dir,\n                                                            val_dir,\n                                                            batch_size)\n\nepoch_history = []\naccuracy_history_train = []\naccuracy_history_val = []\ntrain_model(device,model,\n            loss, \n            optimizer, \n            scheduler, \n            accuracy_history_train,\n            accuracy_history_val,\n            epoch_history,\n            train_dataloader,\n            val_dataloader,\n            num_of_epochs)\n\nprint(max(accuracy_history_train))\nprint(max(accuracy_history_val))\nplt.figure(figsize=(20,12))\nplt.plot(epoch_history ,accuracy_history_train,label = 'train')\nplt.plot(epoch_history ,accuracy_history_val,label = 'val')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-06-16T18:25:44.075071Z","iopub.execute_input":"2025-06-16T18:25:44.075386Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 158MB/s] \n 13%|█▎        | 8/60 [00:19<02:01,  2.33s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"for ind, (inputs, labels) in enumerate(train_dataloader):\n    #print(inputs.shape)\n    y_onehot = torch.zeros(batch_size, len(class_names))\n    y_onehot.scatter_(1, labels.unsqueeze(dim=1), 1)\n    print(y_onehot)\n    print(y_onehot.shape)\n    print(labels)\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from screeninfo import get_monitors\n\nfor i, monitor in enumerate(get_monitors()):\n    print(f\"Monitor {i + 1}: {monitor.width}x{monitor.height} (x={monitor.x}, y={monitor.y})\")\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Monitor 1: 1920x1200 (x=0, y=0)\n","Monitor 2: 1920x1080 (x=-1920, y=0)\n"]}],"execution_count":5},{"cell_type":"code","source":"res = get_monitors()[0]","metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":"res[0].height","metadata":{},"outputs":[{"data":{"text/plain":["1200"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"execution_count":9}]}